Результаты модели: R2 = 0.61113, MAE = 0.50595. Однако, эти результаты не отражают реальной производительности модели, тк наблюдается сильный дисбаланс классов, к тому же я применял метод "smote". Класс 6 составляет 70% датасета, в то время как классы 3, 4, 8 и 9 составляют 2%, 5%, 10% и 3% соответственно.

Почему плохие результаты и варианты их улучшения:
1. Самая главная проблема - это датасет, а точнее огромнейший дисбаланс классов.
Я пробовал и генерацию новых данных (через метод "smote") и сброс весов через class_weight в sklearn, но почти ничего не менялось.
Можно также упрощать саму модельку, убрав слои, но и это не дает нам почти никаких изменений. 
Также пробовал и метод "RandomUnderSampler", чтобы сократить класс 6, но тогда мы теряем достаточно много данных.


2. Методы решения:
2.1 Самый распространенный метод - пополнение датасета для уменьшения дисбаланса классов, чтобы хоть как-то избавиться от дисбаланса.
2.2 Попробовать сгенирировать синтетические данные, через какую-нибудь gpt-model, при правильном промпте, данные почти не будут отличаться от реальных и это поможет нам. Не совсем честно, но я слышал, что такое применяли в проде и это спасало. 
2.3 Как только решим проблему сильного дисбаланса, можно еще создать еще пару фичей 2-4, чтоб модель лучше обучалась. 
2.4 Настройка гиперпараметров: После добавления новых фичей можно заняться подбором оптимальных значений learning rate, weight_decay и числа эпох, чтобы добиться максимальной точности модели.